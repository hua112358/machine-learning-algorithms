{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$Sigmoid函数\\\\ \n",
       "\\sigma(z) = \\frac{1}{1+e^{-z}}\\\\ \n",
       "将回归值从(-\\infty,+\\infty)映射到(0,1)\\\\ \n",
       "z = W^TX$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$Sigmoid函数\\\\ \n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}\\\\ \n",
    "将回归值从(-\\infty,+\\infty)映射到(0,1)\\\\ \n",
    "z = W^TX$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$梯度下降法\\\\ \n",
       "基本思想：要找到某个函数的最小值，最好的方法是沿着该函数的梯度方向寻找。\\\\ \n",
       "梯度：\\nabla f(x,y) = \\dbinom{\\frac{\\partial f(x,y)}{\\partial x}}{\\frac{\\partial f(x,y)}{\\partial y}}\\\\\n",
       "意味着向x方向移动\\frac{\\partial f(x,y)}{\\partial x}，向y方向移动\\frac{\\partial f(x,y)}{\\partial y}\\\\ \n",
       "迭代公式：W := \\alpha \\nabla_Wf(W)$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$梯度下降法\\\\ \n",
    "基本思想：要找到某个函数的最小值，最好的方法是沿着该函数的梯度方向寻找。\\\\ \n",
    "梯度：\\nabla f(x,y) = \\dbinom{\\frac{\\partial f(x,y)}{\\partial x}}{\\frac{\\partial f(x,y)}{\\partial y}}\\\\\n",
    "意味着向x方向移动\\frac{\\partial f(x,y)}{\\partial x}，向y方向移动\\frac{\\partial f(x,y)}{\\partial y}\\\\ \n",
    "迭代公式：W := \\alpha \\nabla_Wf(W)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$假设函数\\\\ \n",
       "h_\\theta(x) = g(\\theta^Tx) = \\frac{1}{1+e^{-\\theta^Tx}}\\\\ \n",
       "h_\\theta(x)代表y取1的概率\\\\\n",
       "当h_\\theta(x)\\geq0.5，即\\theta^Tx\\geq0时，y=1；当h_\\theta(x)<0.5,即\\theta^Tx<0时，y=0 $"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$假设函数\\\\ \n",
    "h_\\theta(x) = g(\\theta^Tx) = \\frac{1}{1+e^{-\\theta^Tx}}\\\\ \n",
    "h_\\theta(x)代表y取1的概率\\\\\n",
    "当h_\\theta(x)\\geq0.5，即\\theta^Tx\\geq0时，y=1；当h_\\theta(x)<0.5,即\\theta^Tx<0时，y=0 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$对于线性回归\\\\\n",
       "代价函数\\\\ \n",
       "Cost(h_\\theta(x),y) = \\frac12(h_\\theta(x)-y)^2\\\\ \n",
       "总代价函数\\\\ \n",
       "J(\\theta) = \\frac1m\\sum\\limits_{i=1}^{m}Cost(h_\\theta(x^i),y^i) $"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$对于线性回归\\\\\n",
    "代价函数\\\\ \n",
    "Cost(h_\\theta(x),y) = \\frac12(h_\\theta(x)-y)^2\\\\ \n",
    "总代价函数\\\\ \n",
    "J(\\theta) = \\frac1m\\sum\\limits_{i=1}^{m}Cost(h_\\theta(x^i),y^i) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$对于逻辑回归\\\\\n",
       "应用线性回归的代价函数和总代价函数会得到一个非凸函数，使得梯度下降法等最优化算法只能找到局部最优解，而不是全局最优解\\\\$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$对于逻辑回归\\\\\n",
    "应用线性回归的代价函数和总代价函数会得到一个非凸函数，使得梯度下降法等最优化算法只能找到局部最优解，而不是全局最优解\\\\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$对于逻辑回归\\\\\n",
       "代价函数\\\\\n",
       "Cost(h_\\theta(x),y) = \\begin{cases}-log(h_\\theta(x))\\ \\ \\ \\ \\ \\ \\ \\ if\\ y = 1\\\\-log(1-h_\\theta(x))\n",
       "\\ if\\ y = 0\\end{cases} = -ylog(h_\\theta(x))-(1-y)log(1-h_\\theta(x)) \\\\\n",
       "当y=1:\\\\\n",
       "h_\\theta(x) = 1时，Cost(h_\\theta(x),y) = 0，表示预测完全准确；\n",
       "h_\\theta(x) = 0时，Cost(h_\\theta(x),y) = +\\infty，表示预测完全错误\\\\\n",
       "当y=0:\\\\\n",
       "h_\\theta(x) = 1时，Cost(h_\\theta(x),y) = +\\infty，表示预测完全错误；\n",
       "h_\\theta(x) = 0时，Cost(h_\\theta(x),y) = 0，表示预测完全正确 \\\\\n",
       "总代价函数\\\\\n",
       "J(\\theta) = \\frac1m\\sum\\limits_{i=1}^{m}Cost(h_\\theta(x^i),y^i)\n",
       "= -\\frac1m\\sum\\limits_{i=1}^{m}y^ilog(h_\\theta(x^i))+(1-y^i)log(1-h_\\theta(x^i))\\\\\n",
       "J(\\theta)是一个凸函数 $"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$对于逻辑回归\\\\\n",
    "代价函数\\\\\n",
    "Cost(h_\\theta(x),y) = \\begin{cases}-log(h_\\theta(x))\\ \\ \\ \\ \\ \\ \\ \\ if\\ y = 1\\\\-log(1-h_\\theta(x))\n",
    "\\ if\\ y = 0\\end{cases} = -ylog(h_\\theta(x))-(1-y)log(1-h_\\theta(x)) \\\\\n",
    "当y=1:\\\\\n",
    "h_\\theta(x) = 1时，Cost(h_\\theta(x),y) = 0，表示预测完全准确；\n",
    "h_\\theta(x) = 0时，Cost(h_\\theta(x),y) = +\\infty，表示预测完全错误\\\\\n",
    "当y=0:\\\\\n",
    "h_\\theta(x) = 1时，Cost(h_\\theta(x),y) = +\\infty，表示预测完全错误；\n",
    "h_\\theta(x) = 0时，Cost(h_\\theta(x),y) = 0，表示预测完全正确 \\\\\n",
    "总代价函数\\\\\n",
    "J(\\theta) = \\frac1m\\sum\\limits_{i=1}^{m}Cost(h_\\theta(x^i),y^i)\n",
    "= -\\frac1m\\sum\\limits_{i=1}^{m}y^ilog(h_\\theta(x^i))+(1-y^i)log(1-h_\\theta(x^i))\\\\\n",
    "J(\\theta)是一个凸函数 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$用梯度下降法求min\\ J(\\theta)\\\\\n",
       "迭代公式\\\\\n",
       "\\theta := \\theta - \\alpha\\nabla_\\theta J(\\theta) = \\theta - \\alpha\\frac{\\partial J(\\theta)}{\\theta}\n",
       "= \\theta-\\alpha\\frac1m\\sum\\limits_{i=1}^{m}(h_\\theta(x^i)-y^i)x^i \\\\\n",
       "\\theta = [\\theta_1,\\theta_2,...,\\theta_j,...\\theta_n]\\\\\n",
       "\\theta_j := \\theta_j - \\alpha\\frac{\\partial J(\\theta)}{\\theta_j} = \\theta_j - \\alpha\\frac1m\\sum\\limits_{i=1}^{m}(h_\\theta(x^i)-y^i)x^i_j \\\\\n",
       "注：\\\\\n",
       "一、所有的\\theta_j必须同时更新\\\\\n",
       "二、上标i代表样本索引，下标j代表属性索引$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$用梯度下降法求min\\ J(\\theta)\\\\\n",
    "迭代公式\\\\\n",
    "\\theta := \\theta - \\alpha\\nabla_\\theta J(\\theta) = \\theta - \\alpha\\frac{\\partial J(\\theta)}{\\theta}\n",
    "= \\theta-\\alpha\\frac1m\\sum\\limits_{i=1}^{m}(h_\\theta(x^i)-y^i)x^i \\\\\n",
    "\\theta = [\\theta_1,\\theta_2,...,\\theta_j,...\\theta_n]\\\\\n",
    "\\theta_j := \\theta_j - \\alpha\\frac{\\partial J(\\theta)}{\\theta_j} = \\theta_j - \\alpha\\frac1m\\sum\\limits_{i=1}^{m}(h_\\theta(x^i)-y^i)x^i_j \\\\\n",
    "注：\\\\\n",
    "一、所有的\\theta_j必须同时更新\\\\\n",
    "二、上标i代表样本索引，下标j代表属性索引$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$过拟合与正则化\\\\\n",
       "解决过拟合问题的方法\\\\\n",
       "一、减少特征数量\\\\\n",
       "二、正则化（减小特征系数\\theta的值）$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$过拟合与正则化\\\\\n",
    "解决过拟合问题的方法\\\\\n",
    "一、减少特征数量\\\\\n",
    "二、正则化（减小特征系数\\theta的值）$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$正则化即减小特征系数\\theta的值，降低\\theta的对模型影响，使得假设函数更简单，曲线更平滑，因此降低了过拟合的可能。$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$正则化即减小特征系数\\theta的值，降低\\theta的对模型影响，使得假设函数更简单，曲线更平滑，因此降低了过拟合的可能。$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$由于不知道哪些特征导致过拟合，因此减小所有特征系数\\theta的值。\\\\\n",
       "对线性回归进行正则化\\\\\n",
       "总代价函数\\\\\n",
       "J(\\theta) = \\frac1{2m}(\\sum\\limits_{i = 1}^{m}(h_\\theta(x^i)-y^i)^2+\\lambda\\sum\\limits_{j=1}^{n}\\theta_j^2) $"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$由于不知道哪些特征导致过拟合，因此减小所有特征系数\\theta的值。\\\\\n",
    "对线性回归进行正则化\\\\\n",
    "总代价函数\\\\\n",
    "J(\\theta) = \\frac1{2m}(\\sum\\limits_{i = 1}^{m}(h_\\theta(x^i)-y^i)^2+\\lambda\\sum\\limits_{j=1}^{n}\\theta_j^2) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$对正则化后的总代价函数用梯度下降法求min\\ J(\\theta)\\\\\n",
       "迭代公式\\\\\n",
       "\\theta_0 := \\theta_0 - \\alpha \\frac1m\\sum\\limits_{i = 1}^{m}(h_\\theta(x^i)-y^i)x_0^i\\\\\n",
       "\\theta_j := \\theta_j - \\alpha \\frac1m(\\sum\\limits_{i = 1}^{m}(h_\\theta(x^i)-y^i)x_j^i+\\lambda\\theta_j)\n",
       " = \\theta_j(1-\\alpha\\frac{\\lambda}{m})-\\alpha\\frac1m\\sum\\limits_{i = 1}^{m}(h_\\theta(x^i)-y^i)x_j^i \\\\\n",
       "注：\\theta_0不参与正则化$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$对正则化后的总代价函数用梯度下降法求min\\ J(\\theta)\\\\\n",
    "迭代公式\\\\\n",
    "\\theta_0 := \\theta_0 - \\alpha \\frac1m\\sum\\limits_{i = 1}^{m}(h_\\theta(x^i)-y^i)x_0^i\\\\\n",
    "\\theta_j := \\theta_j - \\alpha \\frac1m(\\sum\\limits_{i = 1}^{m}(h_\\theta(x^i)-y^i)x_j^i+\\lambda\\theta_j)\n",
    " = \\theta_j(1-\\alpha\\frac{\\lambda}{m})-\\alpha\\frac1m\\sum\\limits_{i = 1}^{m}(h_\\theta(x^i)-y^i)x_j^i \\\\\n",
    "注：\\theta_0不参与正则化$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$对逻辑回归进行正则化\\\\\n",
       "总代价函数\\\\\n",
       "J(\\theta) = -\\frac1m\\sum\\limits_{i=1}^{m}y^ilog(h_\\theta(x^i))+(1-y^i)log(1-h_\\theta(x^i))\n",
       "+\\frac{\\lambda}{2m}\\sum\\limits_{j = 1}^{n}\\theta_j^2 $"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$对逻辑回归进行正则化\\\\\n",
    "总代价函数\\\\\n",
    "J(\\theta) = -\\frac1m\\sum\\limits_{i=1}^{m}y^ilog(h_\\theta(x^i))+(1-y^i)log(1-h_\\theta(x^i))\n",
    "+\\frac{\\lambda}{2m}\\sum\\limits_{j = 1}^{n}\\theta_j^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$对正则化后的总代价函数用梯度下降法求min\\ J(\\theta)\\\\\n",
       "迭代公式\\\\\n",
       "\\theta_0 = \\theta_0 - \\alpha\\frac1m\\sum\\limits_{i = 1}^{m}(h_\\theta(x^i)-y^i)x_0^i\\\\\n",
       "\\theta_j = \\theta_j - \\alpha(\\frac1m\\sum\\limits_{i = 1}^{m}(h_\\theta(x^i)-y^i)x_j^i+\\frac{\\lambda}{m}\\theta_j)\n",
       "= \\theta_j(1-\\alpha\\frac{\\lambda}m)-\\alpha\\frac1m\\sum\\limits_{i = 1}^m(h_\\theta(x^i)-y^i)x_j^i $"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$对正则化后的总代价函数用梯度下降法求min\\ J(\\theta)\\\\\n",
    "迭代公式\\\\\n",
    "\\theta_0 = \\theta_0 - \\alpha\\frac1m\\sum\\limits_{i = 1}^{m}(h_\\theta(x^i)-y^i)x_0^i\\\\\n",
    "\\theta_j = \\theta_j - \\alpha(\\frac1m\\sum\\limits_{i = 1}^{m}(h_\\theta(x^i)-y^i)x_j^i+\\frac{\\lambda}{m}\\theta_j)\n",
    "= \\theta_j(1-\\alpha\\frac{\\lambda}m)-\\alpha\\frac1m\\sum\\limits_{i = 1}^m(h_\\theta(x^i)-y^i)x_j^i $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$其他优化算法\\\\\n",
       "一、Conjugate gradient method(共轭梯度法)\\\\\n",
       "二、Quasi-Newton method(拟牛顿法)\\\\\n",
       "三、BFGS method\\\\\n",
       "四、L-BFGS(Limited-memory BFGS)\\\\\n",
       "优点：不需要手动的选择步长；通常比梯度下降算法快\\\\\n",
       "缺点：更复杂$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$其他优化算法\\\\\n",
    "一、Conjugate gradient method(共轭梯度法)\\\\\n",
    "二、Quasi-Newton method(拟牛顿法)\\\\\n",
    "三、BFGS method\\\\\n",
    "四、L-BFGS(Limited-memory BFGS)\\\\\n",
    "优点：不需要手动的选择步长；通常比梯度下降算法快\\\\\n",
    "缺点：更复杂$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\lambda越大，惩罚力度越大，特征系数\\theta越倾向于0$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$\\lambda越大，惩罚力度越大，特征系数\\theta越倾向于0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
