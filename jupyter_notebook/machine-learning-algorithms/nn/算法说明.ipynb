{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$M-P神经元模型\\\\\n",
       "y = f(\\sum\\limits_{i=1}^{n}w_ix_i-\\theta)\\\\\n",
       "其中\\theta是阈值，f是激活函数\\\\\n",
       "$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$M-P神经元模型\\\\\n",
    "y = f(\\sum\\limits_{i=1}^{n}w_ix_i-\\theta)\\\\\n",
    "其中\\theta是阈值，f是激活函数\\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$感知机由两层神经元组成\\\\\n",
       "可以实现线性可分的与、或、非等逻辑运算，但是不能解决线性不可分的异或运算\\\\\n",
       "学习(训练)规则：w_i:=w_i+\\delta_i,\\delta_i=\\alpha(y-\\hat{y})x_i\\\\\n",
       "即Delta学习，若估计值小于真实值，增加正项权重，减小负项权重。\\alpha是学习率。$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$感知机由两层神经元组成\\\\\n",
    "可以实现线性可分的与、或、非等逻辑运算，但是不能解决线性不可分的异或运算\\\\\n",
    "学习(训练)规则：w_i:=w_i+\\delta_i,\\delta_i=\\alpha(y-\\hat{y})x_i\\\\\n",
    "即Delta学习，若估计值小于真实值，增加正项权重，减小负项权重。\\alpha是学习率。$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$要解决非线性问题，就要使用多层神经网络\\\\\n",
       "前馈神经网络：每层神经元都与下一层全互连，不存在同层和跨层连接\\\\$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$要解决非线性问题，就要使用多层神经网络\\\\\n",
    "前馈神经网络：每层神经元都与下一层全互连，不存在同层和跨层连接\\\\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$BP算法\\\\\n",
       "多层神经网络更加复杂，简单的感知机学习(训练)规则Delta规则不够用，出现了BP算法\\\\\n",
       "因此BP算法不是神经网络，而是一种学习(训练)神经网络的规则，可用于不同的\\\\\n",
       "神经网络，如：前馈神经网络，递归神经网络，但一般说BP网络指的是BP前馈神经网络。$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$BP算法\\\\\n",
    "多层神经网络更加复杂，简单的感知机学习(训练)规则Delta规则不够用，出现了BP算法\\\\\n",
    "因此BP算法不是神经网络，而是一种学习(训练)神经网络的规则，可用于不同的\\\\\n",
    "神经网络，如：前馈神经网络，递归神经网络，但一般说BP网络指的是BP前馈神经网络。$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$BP算法\\\\\n",
       "一个两层(只有一个隐藏层)前馈神经网络，用BP算法进行学习\\\\\n",
       "设：\\\\\n",
       "输入层(x_1,...,x_i,...,x_d)\\\\\n",
       "隐藏层的输入(\\alpha_1,...,\\alpha_h,...,\\alpha_q),\\alpha_h = \\sum\\limits_{i=1}^{d}v_{ih}x_i\\\\\n",
       "隐藏层的输出(b_1,...,b_h,...,b_q),b_h = f(\\alpha_h-\\gamma_h)\\\\\n",
       "输出层的输入(\\beta_1,...,\\beta_j,...,\\beta_l),\\beta_j=\\sum\\limits_{h=1}^{q}w_{hj}b_h\\\\\n",
       "输出层的输出(y_1,...,y_j,...,y_l),y_j=f(\\beta_j-\\theta_j)\\\\\n",
       "其中：\\\\\n",
       "f是激活函数，w和v分别是隐藏层到输出层和输入层到隐藏层的权重\\\\\n",
       "\\theta和\\gamma分别是输出层和隐藏层的阈值\\\\\n",
       "对于元组(X^k,y^k)：\\\\\n",
       "估计值为：\\hat{y}^k=(\\hat{y}^k_1,...,\\hat{y}^k_j,...,\\hat{y}^k_l)\\\\\n",
       "均方误差为：E^K=\\frac12\\sum\\limits_{j=1}^{l}(\\hat{y}^k_j-y^k_j)^2\\\\\n",
       "采用梯度下降法最小化均方误差\\\\\n",
       "对神经网络中任意未知参数\\mu,学习规则为：\\mu:=\\mu+\\Delta\\mu,\\Delta\\mu=-\\eta\\frac{\\partial{E_k}}{\\partial{\\mu}}\\\\\n",
       "向负梯度方向下降$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$BP算法\\\\\n",
    "一个两层(只有一个隐藏层)前馈神经网络，用BP算法进行学习\\\\\n",
    "设：\\\\\n",
    "输入层(x_1,...,x_i,...,x_d)\\\\\n",
    "隐藏层的输入(\\alpha_1,...,\\alpha_h,...,\\alpha_q),\\alpha_h = \\sum\\limits_{i=1}^{d}v_{ih}x_i\\\\\n",
    "隐藏层的输出(b_1,...,b_h,...,b_q),b_h = f(\\alpha_h-\\gamma_h)\\\\\n",
    "输出层的输入(\\beta_1,...,\\beta_j,...,\\beta_l),\\beta_j=\\sum\\limits_{h=1}^{q}w_{hj}b_h\\\\\n",
    "输出层的输出(y_1,...,y_j,...,y_l),y_j=f(\\beta_j-\\theta_j)\\\\\n",
    "其中：\\\\\n",
    "f是激活函数，w和v分别是隐藏层到输出层和输入层到隐藏层的权重\\\\\n",
    "\\theta和\\gamma分别是输出层和隐藏层的阈值\\\\\n",
    "对于元组(X^k,y^k)：\\\\\n",
    "估计值为：\\hat{y}^k=(\\hat{y}^k_1,...,\\hat{y}^k_j,...,\\hat{y}^k_l)\\\\\n",
    "均方误差为：E^K=\\frac12\\sum\\limits_{j=1}^{l}(\\hat{y}^k_j-y^k_j)^2\\\\\n",
    "采用梯度下降法最小化均方误差\\\\\n",
    "对神经网络中任意未知参数\\mu,学习规则为：\\mu:=\\mu+\\Delta\\mu,\\Delta\\mu=-\\eta\\frac{\\partial{E_k}}{\\partial{\\mu}}\\\\\n",
    "向负梯度方向下降$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\Delta{w_{hj}}=-\\eta\\frac{\\partial{E_k}}{\\partial{w_{hj}}}\\\\\n",
       "\\frac{\\partial{E_k}}{\\partial{w_{hj}}}\n",
       "=\\frac{\\partial{E_k}}{\\partial{\\hat{y}_j^k}}\n",
       "\\frac{\\partial{\\hat{y}_j^k}}{\\partial{\\beta_j}}\n",
       "\\frac{\\partial{\\beta_j}}{\\partial{w_{hj}}}=\n",
       "(\\hat{y}^k_j-y^k_j)(\\hat{y}_j^k(1-\\hat{y}_j^k))(b_h)\\\\\n",
       "计算\\frac{\\partial{\\hat{y}_j^k}}{\\partial{\\beta_j}}用sigmoid函数性质：\n",
       "f^{'}=f(1-f)$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$\\Delta{w_{hj}}=-\\eta\\frac{\\partial{E_k}}{\\partial{w_{hj}}}\\\\\n",
    "\\frac{\\partial{E_k}}{\\partial{w_{hj}}}\n",
    "=\\frac{\\partial{E_k}}{\\partial{\\hat{y}_j^k}}\n",
    "\\frac{\\partial{\\hat{y}_j^k}}{\\partial{\\beta_j}}\n",
    "\\frac{\\partial{\\beta_j}}{\\partial{w_{hj}}}=\n",
    "(\\hat{y}^k_j-y^k_j)(\\hat{y}_j^k(1-\\hat{y}_j^k))(b_h)\\\\\n",
    "计算\\frac{\\partial{\\hat{y}_j^k}}{\\partial{\\beta_j}}用sigmoid函数性质：\n",
    "f^{'}=f(1-f)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\Delta{\\theta_j}=-\\eta\\frac{\\partial{E_k}}{\\partial{\\theta_j}}\\\\\n",
       "\\frac{\\partial{E_k}}{\\partial{\\theta_j}}\n",
       "=\\frac{\\partial{E_k}}{\\partial{\\hat{y}_j^k}}\n",
       "\\frac{\\partial{\\hat{y}_j^k}}{\\partial{\\theta_j}}=\n",
       "(\\hat{y}^k_j-y^k_j)(\\hat{y}_j^k(1-\\hat{y}_j^k)(-1))\\\\\n",
       "计算\\frac{\\partial{\\hat{y}_j^k}}{\\partial{\\theta_j}}用sigmoid函数性质：\n",
       "f^{'}=f(1-f)$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$\\Delta{\\theta_j}=-\\eta\\frac{\\partial{E_k}}{\\partial{\\theta_j}}\\\\\n",
    "\\frac{\\partial{E_k}}{\\partial{\\theta_j}}\n",
    "=\\frac{\\partial{E_k}}{\\partial{\\hat{y}_j^k}}\n",
    "\\frac{\\partial{\\hat{y}_j^k}}{\\partial{\\theta_j}}=\n",
    "(\\hat{y}^k_j-y^k_j)(\\hat{y}_j^k(1-\\hat{y}_j^k)(-1))\\\\\n",
    "计算\\frac{\\partial{\\hat{y}_j^k}}{\\partial{\\theta_j}}用sigmoid函数性质：\n",
    "f^{'}=f(1-f)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\Delta{v_{ih}}=-\\eta\\frac{\\partial{E_k}}{\\partial{v_{ih}}}\\\\\n",
       "\\frac{\\partial{E_k}}{\\partial{v_{ih}}}\n",
       "=\\sum\\limits_{j=1}^{l}\\frac{\\partial{E_k}}{\\partial{\\hat{y}_j^k}}\n",
       "\\frac{\\partial{\\hat{y}_j^k}}{\\partial{\\beta_j}}\n",
       "\\frac{\\partial{\\beta_j}}{\\partial{b_h}}\n",
       "\\frac{\\partial{b_h}}{\\partial{\\alpha_h}}\n",
       "\\frac{\\partial{\\alpha_h}}{\\partial{v_{ih}}}=\n",
       "\\sum\\limits_{j=1}^{l}(\\hat{y}^k_j-y^k_j)(\\hat{y}_j^k(1-\\hat{y}_j^k))(w_{hj})(b_h(1-b_h))(x_i)\\\\\n",
       "计算\\frac{\\partial{\\hat{y}_j^k}}{\\partial{\\beta_j}}和\n",
       "\\frac{\\partial{b_h}}{\\partial{\\alpha_h}}用sigmoid函数性质：\n",
       "f^{'}=f(1-f)$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$\\Delta{v_{ih}}=-\\eta\\frac{\\partial{E_k}}{\\partial{v_{ih}}}\\\\\n",
    "\\frac{\\partial{E_k}}{\\partial{v_{ih}}}\n",
    "=\\sum\\limits_{j=1}^{l}\\frac{\\partial{E_k}}{\\partial{\\hat{y}_j^k}}\n",
    "\\frac{\\partial{\\hat{y}_j^k}}{\\partial{\\beta_j}}\n",
    "\\frac{\\partial{\\beta_j}}{\\partial{b_h}}\n",
    "\\frac{\\partial{b_h}}{\\partial{\\alpha_h}}\n",
    "\\frac{\\partial{\\alpha_h}}{\\partial{v_{ih}}}=\n",
    "\\sum\\limits_{j=1}^{l}(\\hat{y}^k_j-y^k_j)(\\hat{y}_j^k(1-\\hat{y}_j^k))(w_{hj})(b_h(1-b_h))(x_i)\\\\\n",
    "计算\\frac{\\partial{\\hat{y}_j^k}}{\\partial{\\beta_j}}和\n",
    "\\frac{\\partial{b_h}}{\\partial{\\alpha_h}}用sigmoid函数性质：\n",
    "f^{'}=f(1-f)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\Delta{\\gamma_h}=-\\eta\\frac{\\partial{E_k}}{\\partial{\\gamma_h}}\\\\\n",
       "\\frac{\\partial{E_k}}{\\partial{\\gamma_h}}\n",
       "=\\sum\\limits_{j=1}^{l}\\frac{\\partial{E_k}}{\\partial{\\hat{y}_j^k}}\n",
       "\\frac{\\partial{\\hat{y}_j^k}}{\\partial{\\beta_j}}\n",
       "\\frac{\\partial{\\beta_j}}{\\partial{b_h}}\n",
       "\\frac{\\partial{b_h}}{\\partial{\\gamma_h}}=\n",
       "\\sum\\limits_{j=1}^{l}(\\hat{y}^k_j-y^k_j)(\\hat{y}_j^k(1-\\hat{y}_j^k))(w_{hj})(b_h(1-b_h)(-1))\\\\\n",
       "计算\\frac{\\partial{\\hat{y}_j^k}}{\\partial{\\beta_j}}和\n",
       "\\frac{\\partial{b_h}}{\\partial{\\gamma_h}}用sigmoid函数性质：\n",
       "f^{'}=f(1-f)$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$\\Delta{\\gamma_h}=-\\eta\\frac{\\partial{E_k}}{\\partial{\\gamma_h}}\\\\\n",
    "\\frac{\\partial{E_k}}{\\partial{\\gamma_h}}\n",
    "=\\sum\\limits_{j=1}^{l}\\frac{\\partial{E_k}}{\\partial{\\hat{y}_j^k}}\n",
    "\\frac{\\partial{\\hat{y}_j^k}}{\\partial{\\beta_j}}\n",
    "\\frac{\\partial{\\beta_j}}{\\partial{b_h}}\n",
    "\\frac{\\partial{b_h}}{\\partial{\\gamma_h}}=\n",
    "\\sum\\limits_{j=1}^{l}(\\hat{y}^k_j-y^k_j)(\\hat{y}_j^k(1-\\hat{y}_j^k))(w_{hj})(b_h(1-b_h)(-1))\\\\\n",
    "计算\\frac{\\partial{\\hat{y}_j^k}}{\\partial{\\beta_j}}和\n",
    "\\frac{\\partial{b_h}}{\\partial{\\gamma_h}}用sigmoid函数性质：\n",
    "f^{'}=f(1-f)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$由于BP网络的强大的表现能力，经常会出现过拟合的情况，解决方法有两种:\\\\\n",
       "一、早停\\\\\n",
       "将数据集分为训练集和测试集，当训练集错误率下降，测试集错误率上升时则停止训练\\\\\n",
       "二、正则化\\\\\n",
       "给均方误差函数加上一个正则项，例如：\\\\\n",
       "E = \\lambda\\frac1m\\sum\\limits_{k=1}^{m}E^k+(1-\\lambda)\\sum\\limits_{i=1}w_i^2$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$由于BP网络的强大的表现能力，经常会出现过拟合的情况，解决方法有两种:\\\\\n",
    "一、早停\\\\\n",
    "将数据集分为训练集和测试集，当训练集错误率下降，测试集错误率上升时则停止训练\\\\\n",
    "二、正则化\\\\\n",
    "给均方误差函数加上一个正则项，例如：\\\\\n",
    "E = \\lambda\\frac1m\\sum\\limits_{k=1}^{m}E^k+(1-\\lambda)\\sum\\limits_{i=1}w_i^2\\\\\n",
    "w代表所有权重和阈值$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$解决非全局最优的方法\\\\\n",
       "一、对参数进行多次不同的初始化\\\\\n",
       "二、模拟退火\\\\\n",
       "三、随机梯度下降$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$解决非全局最优的方法\\\\\n",
    "一、对参数进行多次不同的初始化\\\\\n",
    "二、模拟退火\\\\\n",
    "三、随机梯度下降$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$其他神经网络\\\\\n",
       "一、rbf神经网络\\\\\n",
       "二、art神经网络\\\\\n",
       "三、som神经网络\\\\\n",
       "四、级联相关网络\\\\\n",
       "五、elman神经网络(递归神经网络)\\\\\n",
       "六、boltzmann机\\\\$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$其他神经网络\\\\\n",
    "一、rbf神经网络\\\\\n",
    "二、art神经网络\\\\\n",
    "三、som神经网络\\\\\n",
    "四、级联相关网络\\\\\n",
    "五、elman神经网络(递归神经网络)\\\\\n",
    "六、boltzmann机\\\\$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
