{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导入数据\n",
    "import feedparser\n",
    "ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "sf = feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义文本转换函数\n",
    "#re.split()比str.split()更实用\n",
    "# def text_parse(rss_text):\n",
    "#     import re\n",
    "#     word_list = re.split(r'\\W*',rss_text)\n",
    "#     return [word.lower() for word in word_list if len(word)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#从rss源中提取正常文本列表\n",
    "# ny_X = []\n",
    "ny_y = []\n",
    "ny_X_rss = []\n",
    "m_examples_ny = len(ny['entries'])\n",
    "for i in range(m_examples_ny):\n",
    "    instance = ny['entries'][i]\n",
    "    rss_text = instance['summary']\n",
    "#     ny_X.append(text_parse(rss_text))\n",
    "    ny_y.append('ny')\n",
    "    ny_X_rss.append(rss_text)\n",
    "\n",
    "# sf_X = []\n",
    "sf_y = []\n",
    "sf_X_rss = []\n",
    "m_examples_sf = len(sf['entries'])\n",
    "for i in range(m_examples_sf):\n",
    "    instance = sf['entries'][i]\n",
    "    rss_text = instance['summary']\n",
    "#     sf_X.append(text_parse(rss_text))\n",
    "    sf_y.append('sf')\n",
    "    sf_X_rss.append(rss_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#连接两个文本列表\n",
    "# X = ny_X.copy()\n",
    "y = ny_y.copy()\n",
    "# for instance in sf_X:\n",
    "#     X.append(instance)\n",
    "for class_of_instance in sf_y:\n",
    "    y.append(class_of_instance)\n",
    "X_rss = ny_X_rss.copy()\n",
    "for instance in sf_X_rss:\n",
    "    X_rss.append(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#数据预处理\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "X_transformer = CountVectorizer()\n",
    "X_transformed = X_transformer.fit_transform(X_rss)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "y_transformer = LabelEncoder()\n",
    "y_transformed = y_transformer.fit_transform(y)\n",
    "class_names = y_transformer.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average score of count vectorizer is 38.2%\n",
      "the average score of tfidf vectorizer is 44.0%\n",
      "the average score of hash vectorizer is 37.5%\n"
     ]
    }
   ],
   "source": [
    "#构建、评估分类器\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,HashingVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "countvectorizer_multinomialnb_pipeline = Pipeline([('countvectorizer',CountVectorizer()),\n",
    "                                   ('multinomialnb',MultinomialNB())])\n",
    "tfidfvectorizer_multinomialnb_pipeline = Pipeline([('tfidfvectorizer',TfidfVectorizer()),\n",
    "                                             ('multinomialnb',MultinomialNB())])\n",
    "hashingvectorizer_multinomialnb_pipeline = Pipeline([('hashingvectorizer',HashingVectorizer(non_negative = True)),\n",
    "                                               ('multinomialnb',MultinomialNB())])\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores_countvectorizer = cross_val_score(countvectorizer_multinomialnb_pipeline,X_rss,y,scoring = 'accuracy')\n",
    "scores_tfidfvectorizer = cross_val_score(tfidfvectorizer_multinomialnb_pipeline,X_rss,y,scoring = 'accuracy')\n",
    "scores_hashingvectorizer = cross_val_score(hashingvectorizer_multinomialnb_pipeline,X_rss,y,scoring = 'accuracy')\n",
    "import numpy as np\n",
    "print('the average score of count vectorizer is {0:.1f}%'.format(np.mean(scores_countvectorizer)*100))\n",
    "print('the average score of tfidf vectorizer is {0:.1f}%'.format(np.mean(scores_tfidfvectorizer)*100))\n",
    "print('the average score of hash vectorizer is {0:.1f}%'.format(np.mean(scores_hashingvectorizer)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#注：\n",
    "#注释掉的代码意思是不需要该处代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countvectorizer_transformer = CountVectorizer()\n",
    "tfidfvectorizer_transformer = TfidfVectorizer()\n",
    "hashingvectorizer_transformer = HashingVectorizer()\n",
    "X_transformed_count = countvectorizer_transformer.fit_transform(X_rss)\n",
    "X_transformed_tfidf = tfidfvectorizer_transformer.fit_transform(X_rss)\n",
    "X_transformed_hash = hashingvectorizer_transformer.fit_transform(X_rss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 1 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]] (50, 719) {'married': 385, '42': 11, 'year': 709, 'old': 443, 'italian': 327, 'american': 44, 'male': 381, 'looking': 371, 'to': 632, 'connect': 143, 'with': 695, 'an': 46, 'attached': 64, 'woman': 697, 'around': 58, 'my': 419, 'age': 30, 'your': 717, 'ethnicity': 208, 'is': 325, 'unimportant': 654, 'me': 392, 'getting': 262, 'sex': 551, 'simple': 561, 'it': 326, 'not': 431, 'what': 681, 'am': 42, 'for': 243, 'seek': 539, 'intellectual': 317, 'connection': 144, 'like': 357, 'hello': 289, 'black': 88, 'living': 364, 'alone': 35, 'in': 312, 'the': 617, 'bronx': 101, 'four': 244, 'twenty': 649, 'and': 47, 'drink': 189, 'cool': 149, 'very': 663, 'fun': 255, 'female': 229, 'hang': 279, 'joke': 333, 'just': 335, 'have': 284, 'picture': 474, 'would': 705, 'be': 74, 'nice': 425, 'hopefully': 299, 'we': 676, 'could': 150, 'out': 454, 'chill': 120, 'so': 570, 'watching': 674, 'this': 625, 'movie': 416, 'or': 449, 'at': 62, 'least': 348, 'half': 277, 've': 662, 'spent': 583, 'past': 463, 'couple': 151, 'days': 166, 'reading': 508, 'posts': 487, 'responding': 522, 'little': 361, 'luck': 377, 'figured': 234, 'post': 485, 'ad': 23, 'of': 439, 'own': 457, 'peeps': 464, 'that': 615, 'share': 555, '47': 14, 'white': 686, 'easy': 196, 'going': 267, 'likes': 358, 'go': 265, 'flow': 240, 'things': 623, 'was': 672, 'hoping': 300, 'meet': 394, 'enjoy': 203, 'meeting': 395, 'coffee': 135, 'morning': 410, 'afternoon': 28, 'can': 108, 'all': 33, 'sit': 565, 'talk': 608, 'get': 260, 'know': 338, 'each': 195, 'other': 451, 'iam': 307, 'smoke': 569, 'maybe': 391, 'host': 301, 'thats': 616, 'calm': 106, 'wants': 670, 'today': 633, 'send': 543, 'full': 254, 'clean': 126, 'pic': 471, 'serious': 547, 'about': 21, 'up': 657, 'sexy': 553, 'bi': 86, 'femme': 232, 'from': 252, 'seeking': 540, 'feminine': 231, 'females': 230, 'only': 447, 'no': 427, 'couples': 152, '3sums': 9, 'along': 36, 'pretty': 491, 'chick': 119, 'someone': 574, 'who': 687, 'mature': 390, 'don': 181, 'mind': 401, 'if': 308, 'both': 96, 'click': 128, 'ha': 275, 'debris': 169, 'mess': 397, 'say': 534, 'summer': 603, 'yeah': 708, 'oh': 441, 'you': 714, 'where': 683, 'start': 588, 'enter': 204, 'business': 104, 'painting': 458, 'cleaning': 127, 'reasonable': 511, 'rates': 506, 'times': 629, 'convenient': 147, 'us': 658, 'work': 701, 'clothed': 133, 'strap': 596, 'nude': 437, 'non': 428, 'sexual': 552, 'yesterday': 711, 'missed': 405, 'subway': 602, 'stop': 593, 'stand': 587, 'next': 424, 'women': 698, 'whom': 690, 'had': 276, 'amazing': 43, 'unspoken': 656, 'chemistry': 117, 'didn': 173, 'word': 700, 'moved': 414, 'together': 634, 'rhythm': 526, 'train': 640, 'breathing': 100, 'others': 452, 'pheromones': 468, 'want': 668, 'satisfy': 533, 'par': 459, 'massage': 387, 'offer': 440, 'table': 607, 'used': 659, 'cozy': 153, 'setting': 550, 'discretion': 177, 'privacy': 493, 'respected': 520, 'ladies': 339, 'gentlemen': 259, 'are': 56, 'welcomed': 679, 'deep': 170, 'tissue': 630, 'swedish': 605, 'release': 516, 'relax': 514, 'air': 31, 'conditoiner': 141, 'real': 509, 'pics': 473, 'safe': 530, 'drama': 184, 'free': 246, 'worship': 704, 'cute': 160, 'hot': 302, 'trim': 646, 'god': 266, 'between': 85, '18': 2, '28': 5, 'understood': 652, 'superior': 604, 'privilege': 494, 'bow': 98, 'down': 182, 'before': 80, 'grovel': 273, 'feet': 227, 'any': 50, 'way': 675, 'but': 105, 'open': 448, 'treat': 644, '40': 10, 'yr': 718, 'older': 444, 'mental': 396, 'stimulation': 592, 'good': 269, 'hi': 295, 'recently': 512, 'left': 350, 'job': 331, 'because': 76, 'miserable': 404, 'position': 484, 'now': 436, 'find': 235, 'myself': 420, 'time': 628, 'ability': 20, 'improve': 311, 'on': 445, 'vacation': 661, 'school': 535, 'plenty': 480, 'during': 194, 'day': 165, 'retired': 525, 'man': 382, 'trying': 648, 'adjust': 24, 'draw': 185, 'started': 589, 'again': 29, 'pose': 483, 'some': 572, 'one': 446, 'draws': 187, 'too': 637, 'doesn': 179, 'anything': 54, 'than': 614, 'portrait': 482, 'sitting': 566, 'still': 591, 'practice': 488, 'text': 612, 'chat': 116, '24': 3, '365': 8, 'care': 110, 'struggle': 599, 'nyc': 438, 'well': 680, 'here': 292, 'listen': 360, 'help': 290, 'problems': 496, 'stress': 597, 'depression': 171, 'anxiety': 49, 'drug': 192, 'alcohol': 32, 'misuse': 407, 'hour': 303, 'almost': 34, 'language': 342, 'live': 362, 'far': 221, 'heart': 288, 'bed': 78, 'stuy': 601, '31': 7, 'mixed': 408, '10': 0, 'average': 69, 'athletic': 63, 'build': 103, 'minded': 402, 'swm': 606, 'seeks': 541, 'lesbian': 351, 'straight': 595, 'trained': 641, 'as': 59, 'her': 291, 'personal': 467, 'servant': 549, 'much': 417, 'experience': 216, 'doing': 180, 'chores': 121, 'hand': 278, 'machine': 378, 'laundry': 346, 'ironing': 323, 'organize': 450, 'dresser': 188, 'drawers': 186, 'closets': 132, 'also': 39, 'give': 264, 'soothing': 577, 'massages': 388, 'run': 529, 'errands': 205, 'anyt': 53, 'im': 309, 'many': 383, 'girls': 263, 'interested': 319, 'hml': 297, 'href': 305, 'fb': 224, 'stp': 594, '6223055855': 18, 'class': 125, 'showcontact': 560, 'title': 631, 'show': 559, 'contact': 145, 'info': 315, 'hey': 294, 'there': 620, 'cl': 124, 'world': 703, 'working': 702, 'shape': 554, 'need': 422, 'lose': 372, '60lbs': 16, 'else': 199, 'motivated': 412, 'do': 178, 'same': 531, 'already': 38, 'has': 283, 'routine': 528, 'benefit': 82, 'encouraging': 201, 'another': 48, 'th': 613, 'selling': 542, 'alot': 37, 'different': 174, 'products': 497, 'partner': 460, 'make': 379, 'extra': 218, 'spare': 579, 'become': 77, 'better': 84, 'speaker': 581, 'great': 272, 'first': 236, 'level': 354, 'advanced': 25, 'vocabulary': 666, 'best': 83, 'writng': 707, 'stay': 590, 'focused': 242, 'drop': 191, 'line': 359, 'new': 423, 'friends': 250, 'stretching': 598, 'yoga': 713, 'usually': 660, 'place': 476, 'thic': 621, 'small': 567, 'into': 322, '420': 12, 'energy': 202, 'love': 375, 'mine': 403, 'ever': 210, 'felt': 228, 'confused': 142, 'over': 456, 'really': 510, 'most': 411, 'life': 355, 'curse': 159, 'everything': 213, 'everyone': 212, 'sink': 564, 'indefinite': 313, 'random': 505, 'nothingness': 434, 'night': 426, 'either': 198, 'tomorrow': 635, 'please': 479, 'hit': 296, 'prefer': 489, 'asian': 60, 'blonde': 89, 'caucasian': 112, 'lot': 373, 'masculine': 386, 'top': 638, 'explore': 217, 'city': 123, 'veteran': 664, 'educated': 197, 'guy': 274, 'watch': 673, 'sports': 584, 'grab': 271, 'dinner': 175, 'must': 418, 'ddf': 167, 'racial': 503, 'hangups': 280, 'been': 79, '26': 4, 'riverhead': 527, 'laid': 341, 'back': 71, 'lady': 340, 'two': 650, 'trample': 642, 'wanting': 669, 'try': 647, 'problem': 495, 'friendly': 249, 'compensate': 140, 'unless': 655, 'few': 233, 'speak': 580, 'while': 685, 'goof': 270, 'she': 556, 'anymore': 51, 'friend': 248, 'loveless': 376, 'marriage': 384, 'kids': 336, 'separate': 546, 'look': 370, 'eyes': 219, 'matter': 389, 'affair': 27, 'let': 353, 'outgoing': 455, 'reply': 518, 'favorite': 223, 'color': 136, 'years': 710, 'since': 562, 'quaint': 501, 'interesting': 320, 'unfiltered': 653, 'visitor': 665, 'pine': 475, 'wholesome': 689, 'raconteur': 504, 'll': 365, 'appreciate': 55, 'european': 209, 'persona': 466, 'interests': 321, 'theatre': 618, 'books': 94, 'brow': 102, 'flix': 239, 'esp': 206, '50s': 15, '60s': 17, 'stable': 586, 'always': 41, 'their': 619, 'hard': 282, 'herself': 293, 'person': 465, 'preferred': 490, 'partnered': 461, 'seriously': 548, 'our': 453, 'priorities': 492, 'audiobook': 68, 'somebody': 573, 'local': 366, 'audible': 67, 'com': 137, 'credit': 156, 'pick': 472, 'book': 93, 'download': 183, 'phone': 469, 'bluetooth': 91, 'play': 478, 'through': 626, 'think': 624, 'fu': 253, 'wit': 694, 'inside': 316, 'discreet': 176, 'game': 257, 'put': 500, 'headline': 286, 'its': 328, 'joining': 332, 'military': 400, 'lives': 363, 'san': 532, 'francisco': 245, 'contract': 146, 'plan': 477, 'missing': 406, 'special': 582, 'relaxing': 515, 'touch': 639, 'warm': 671, 'body': 92, 'oil': 442, 'relieve': 517, 'calming': 107, 'caressing': 111, 'leaves': 349, 'feeling': 226, 'center': 114, 'search': 537, 'teach': 610, 'dances': 163, 'tapped': 609, 'when': 682, 'younger': 716, 'did': 172, 'ballet': 72, 'crave': 154, 'more': 409, 'movement': 415, 'sensual': 545, 'fascinated': 222, 'latin': 344, 'dancing': 164, 'tribal': 645, 'gets': 261, 'blood': 90, 'how': 304, 'tonight': 636, 'bored': 95, 'change': 115, 'chica': 118, 'late': 343, 'adventure': 26, 'week': 677, 'bite': 87, 'witty': 696, 'drinks': 190, 'professional': 498, 'shoot': 558, 'note': 432, 'beautiful': 75, 'home': 298, 'deal': 168, 'daily': 161, 'tell': 611, 'whole': 688, 'something': 575, 'normally': 430, 'nothing': 433, 'crazy': 155, 'wouldn': 706, 'might': 399, 'cuddling': 158, 'indian': 314, 'wife': 692, 'away': 70, 'thing': 622, 'naughty': 421, 'cuddlin': 157, 'young': 715, 'fit': 237, 'cmt': 134, 'jose': 334, 'clients': 129, 'south': 578, 'bay': 73, 'area': 57, 'message': 398, 'fantasy': 220, 'ass': 61, 'long': 369, 're': 507, 'shirt': 557, 'tie': 627, 'weird': 678, 'lol': 367, 'single': 563, '130': 1, 'lbs': 347, 'travel': 643, 'jewelry': 330, 'making': 380, 'see': 538, 'will': 693, 'respond': 521, 'respect': 519, 'responsible': 524, 'understand': 651, '44': 13, 'latina': 345, 'knight': 337, 'friendship': 251, 'physically': 470, 'intellectually': 318, 'meaningful': 393, 'conversations': 148, 'lightest': 356, 'funny': 256, 'games': 258, 'normal': 429, 'dude': 193, 'happens': 281, 'attraction': 66, 'come': 138, 'box': 99, 'car': 109, 'somewhere': 576, 'lousy': 374, 'hear': 287, 'science': 536, 'stuff': 600, 'poetry': 481, 'sociology': 571, 'novel': 435, 'smart': 568, 'progressive': 499, 'sense': 544, 'humor': 306, 'especially': 207, 'amidst': 45, 'existentially': 214, 'anyone': 52, 'friday': 247, 'dance': 162, 'lesson': 352, 'party': 462, 'jack': 329, 'london': 368, 'square': 585, 'flyer': 241, 'every': 211, 'haven': 285, 'gone': 268, 'yet': 712, 'expand': 215, 'circle': 122, 'flakey': 238, 'race': 502, 'important': 310, 'attitude': 65, 'belay': 81, 'climb': 130, 'ironworks': 324, '30': 6, '8am': 19, 'accountable': 22, 'bouldering': 97, 'why': 691, 'posting': 486, 'won': 699, 'response': 523, 'close': 131, 'although': 40, 'committed': 139, 'relationship': 513, 'feel': 225, 'empty': 200, 'void': 667, 'which': 684, 'ceases': 113, 'motivation': 413}\n",
      "[[ 0.          0.          0.         ...,  0.          0.13271181  0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.121549    0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]] (50, 719) {'married': 385, '42': 11, 'year': 709, 'old': 443, 'italian': 327, 'american': 44, 'male': 381, 'looking': 371, 'to': 632, 'connect': 143, 'with': 695, 'an': 46, 'attached': 64, 'woman': 697, 'around': 58, 'my': 419, 'age': 30, 'your': 717, 'ethnicity': 208, 'is': 325, 'unimportant': 654, 'me': 392, 'getting': 262, 'sex': 551, 'simple': 561, 'it': 326, 'not': 431, 'what': 681, 'am': 42, 'for': 243, 'seek': 539, 'intellectual': 317, 'connection': 144, 'like': 357, 'hello': 289, 'black': 88, 'living': 364, 'alone': 35, 'in': 312, 'the': 617, 'bronx': 101, 'four': 244, 'twenty': 649, 'and': 47, 'drink': 189, 'cool': 149, 'very': 663, 'fun': 255, 'female': 229, 'hang': 279, 'joke': 333, 'just': 335, 'have': 284, 'picture': 474, 'would': 705, 'be': 74, 'nice': 425, 'hopefully': 299, 'we': 676, 'could': 150, 'out': 454, 'chill': 120, 'so': 570, 'watching': 674, 'this': 625, 'movie': 416, 'or': 449, 'at': 62, 'least': 348, 'half': 277, 've': 662, 'spent': 583, 'past': 463, 'couple': 151, 'days': 166, 'reading': 508, 'posts': 487, 'responding': 522, 'little': 361, 'luck': 377, 'figured': 234, 'post': 485, 'ad': 23, 'of': 439, 'own': 457, 'peeps': 464, 'that': 615, 'share': 555, '47': 14, 'white': 686, 'easy': 196, 'going': 267, 'likes': 358, 'go': 265, 'flow': 240, 'things': 623, 'was': 672, 'hoping': 300, 'meet': 394, 'enjoy': 203, 'meeting': 395, 'coffee': 135, 'morning': 410, 'afternoon': 28, 'can': 108, 'all': 33, 'sit': 565, 'talk': 608, 'get': 260, 'know': 338, 'each': 195, 'other': 451, 'iam': 307, 'smoke': 569, 'maybe': 391, 'host': 301, 'thats': 616, 'calm': 106, 'wants': 670, 'today': 633, 'send': 543, 'full': 254, 'clean': 126, 'pic': 471, 'serious': 547, 'about': 21, 'up': 657, 'sexy': 553, 'bi': 86, 'femme': 232, 'from': 252, 'seeking': 540, 'feminine': 231, 'females': 230, 'only': 447, 'no': 427, 'couples': 152, '3sums': 9, 'along': 36, 'pretty': 491, 'chick': 119, 'someone': 574, 'who': 687, 'mature': 390, 'don': 181, 'mind': 401, 'if': 308, 'both': 96, 'click': 128, 'ha': 275, 'debris': 169, 'mess': 397, 'say': 534, 'summer': 603, 'yeah': 708, 'oh': 441, 'you': 714, 'where': 683, 'start': 588, 'enter': 204, 'business': 104, 'painting': 458, 'cleaning': 127, 'reasonable': 511, 'rates': 506, 'times': 629, 'convenient': 147, 'us': 658, 'work': 701, 'clothed': 133, 'strap': 596, 'nude': 437, 'non': 428, 'sexual': 552, 'yesterday': 711, 'missed': 405, 'subway': 602, 'stop': 593, 'stand': 587, 'next': 424, 'women': 698, 'whom': 690, 'had': 276, 'amazing': 43, 'unspoken': 656, 'chemistry': 117, 'didn': 173, 'word': 700, 'moved': 414, 'together': 634, 'rhythm': 526, 'train': 640, 'breathing': 100, 'others': 452, 'pheromones': 468, 'want': 668, 'satisfy': 533, 'par': 459, 'massage': 387, 'offer': 440, 'table': 607, 'used': 659, 'cozy': 153, 'setting': 550, 'discretion': 177, 'privacy': 493, 'respected': 520, 'ladies': 339, 'gentlemen': 259, 'are': 56, 'welcomed': 679, 'deep': 170, 'tissue': 630, 'swedish': 605, 'release': 516, 'relax': 514, 'air': 31, 'conditoiner': 141, 'real': 509, 'pics': 473, 'safe': 530, 'drama': 184, 'free': 246, 'worship': 704, 'cute': 160, 'hot': 302, 'trim': 646, 'god': 266, 'between': 85, '18': 2, '28': 5, 'understood': 652, 'superior': 604, 'privilege': 494, 'bow': 98, 'down': 182, 'before': 80, 'grovel': 273, 'feet': 227, 'any': 50, 'way': 675, 'but': 105, 'open': 448, 'treat': 644, '40': 10, 'yr': 718, 'older': 444, 'mental': 396, 'stimulation': 592, 'good': 269, 'hi': 295, 'recently': 512, 'left': 350, 'job': 331, 'because': 76, 'miserable': 404, 'position': 484, 'now': 436, 'find': 235, 'myself': 420, 'time': 628, 'ability': 20, 'improve': 311, 'on': 445, 'vacation': 661, 'school': 535, 'plenty': 480, 'during': 194, 'day': 165, 'retired': 525, 'man': 382, 'trying': 648, 'adjust': 24, 'draw': 185, 'started': 589, 'again': 29, 'pose': 483, 'some': 572, 'one': 446, 'draws': 187, 'too': 637, 'doesn': 179, 'anything': 54, 'than': 614, 'portrait': 482, 'sitting': 566, 'still': 591, 'practice': 488, 'text': 612, 'chat': 116, '24': 3, '365': 8, 'care': 110, 'struggle': 599, 'nyc': 438, 'well': 680, 'here': 292, 'listen': 360, 'help': 290, 'problems': 496, 'stress': 597, 'depression': 171, 'anxiety': 49, 'drug': 192, 'alcohol': 32, 'misuse': 407, 'hour': 303, 'almost': 34, 'language': 342, 'live': 362, 'far': 221, 'heart': 288, 'bed': 78, 'stuy': 601, '31': 7, 'mixed': 408, '10': 0, 'average': 69, 'athletic': 63, 'build': 103, 'minded': 402, 'swm': 606, 'seeks': 541, 'lesbian': 351, 'straight': 595, 'trained': 641, 'as': 59, 'her': 291, 'personal': 467, 'servant': 549, 'much': 417, 'experience': 216, 'doing': 180, 'chores': 121, 'hand': 278, 'machine': 378, 'laundry': 346, 'ironing': 323, 'organize': 450, 'dresser': 188, 'drawers': 186, 'closets': 132, 'also': 39, 'give': 264, 'soothing': 577, 'massages': 388, 'run': 529, 'errands': 205, 'anyt': 53, 'im': 309, 'many': 383, 'girls': 263, 'interested': 319, 'hml': 297, 'href': 305, 'fb': 224, 'stp': 594, '6223055855': 18, 'class': 125, 'showcontact': 560, 'title': 631, 'show': 559, 'contact': 145, 'info': 315, 'hey': 294, 'there': 620, 'cl': 124, 'world': 703, 'working': 702, 'shape': 554, 'need': 422, 'lose': 372, '60lbs': 16, 'else': 199, 'motivated': 412, 'do': 178, 'same': 531, 'already': 38, 'has': 283, 'routine': 528, 'benefit': 82, 'encouraging': 201, 'another': 48, 'th': 613, 'selling': 542, 'alot': 37, 'different': 174, 'products': 497, 'partner': 460, 'make': 379, 'extra': 218, 'spare': 579, 'become': 77, 'better': 84, 'speaker': 581, 'great': 272, 'first': 236, 'level': 354, 'advanced': 25, 'vocabulary': 666, 'best': 83, 'writng': 707, 'stay': 590, 'focused': 242, 'drop': 191, 'line': 359, 'new': 423, 'friends': 250, 'stretching': 598, 'yoga': 713, 'usually': 660, 'place': 476, 'thic': 621, 'small': 567, 'into': 322, '420': 12, 'energy': 202, 'love': 375, 'mine': 403, 'ever': 210, 'felt': 228, 'confused': 142, 'over': 456, 'really': 510, 'most': 411, 'life': 355, 'curse': 159, 'everything': 213, 'everyone': 212, 'sink': 564, 'indefinite': 313, 'random': 505, 'nothingness': 434, 'night': 426, 'either': 198, 'tomorrow': 635, 'please': 479, 'hit': 296, 'prefer': 489, 'asian': 60, 'blonde': 89, 'caucasian': 112, 'lot': 373, 'masculine': 386, 'top': 638, 'explore': 217, 'city': 123, 'veteran': 664, 'educated': 197, 'guy': 274, 'watch': 673, 'sports': 584, 'grab': 271, 'dinner': 175, 'must': 418, 'ddf': 167, 'racial': 503, 'hangups': 280, 'been': 79, '26': 4, 'riverhead': 527, 'laid': 341, 'back': 71, 'lady': 340, 'two': 650, 'trample': 642, 'wanting': 669, 'try': 647, 'problem': 495, 'friendly': 249, 'compensate': 140, 'unless': 655, 'few': 233, 'speak': 580, 'while': 685, 'goof': 270, 'she': 556, 'anymore': 51, 'friend': 248, 'loveless': 376, 'marriage': 384, 'kids': 336, 'separate': 546, 'look': 370, 'eyes': 219, 'matter': 389, 'affair': 27, 'let': 353, 'outgoing': 455, 'reply': 518, 'favorite': 223, 'color': 136, 'years': 710, 'since': 562, 'quaint': 501, 'interesting': 320, 'unfiltered': 653, 'visitor': 665, 'pine': 475, 'wholesome': 689, 'raconteur': 504, 'll': 365, 'appreciate': 55, 'european': 209, 'persona': 466, 'interests': 321, 'theatre': 618, 'books': 94, 'brow': 102, 'flix': 239, 'esp': 206, '50s': 15, '60s': 17, 'stable': 586, 'always': 41, 'their': 619, 'hard': 282, 'herself': 293, 'person': 465, 'preferred': 490, 'partnered': 461, 'seriously': 548, 'our': 453, 'priorities': 492, 'audiobook': 68, 'somebody': 573, 'local': 366, 'audible': 67, 'com': 137, 'credit': 156, 'pick': 472, 'book': 93, 'download': 183, 'phone': 469, 'bluetooth': 91, 'play': 478, 'through': 626, 'think': 624, 'fu': 253, 'wit': 694, 'inside': 316, 'discreet': 176, 'game': 257, 'put': 500, 'headline': 286, 'its': 328, 'joining': 332, 'military': 400, 'lives': 363, 'san': 532, 'francisco': 245, 'contract': 146, 'plan': 477, 'missing': 406, 'special': 582, 'relaxing': 515, 'touch': 639, 'warm': 671, 'body': 92, 'oil': 442, 'relieve': 517, 'calming': 107, 'caressing': 111, 'leaves': 349, 'feeling': 226, 'center': 114, 'search': 537, 'teach': 610, 'dances': 163, 'tapped': 609, 'when': 682, 'younger': 716, 'did': 172, 'ballet': 72, 'crave': 154, 'more': 409, 'movement': 415, 'sensual': 545, 'fascinated': 222, 'latin': 344, 'dancing': 164, 'tribal': 645, 'gets': 261, 'blood': 90, 'how': 304, 'tonight': 636, 'bored': 95, 'change': 115, 'chica': 118, 'late': 343, 'adventure': 26, 'week': 677, 'bite': 87, 'witty': 696, 'drinks': 190, 'professional': 498, 'shoot': 558, 'note': 432, 'beautiful': 75, 'home': 298, 'deal': 168, 'daily': 161, 'tell': 611, 'whole': 688, 'something': 575, 'normally': 430, 'nothing': 433, 'crazy': 155, 'wouldn': 706, 'might': 399, 'cuddling': 158, 'indian': 314, 'wife': 692, 'away': 70, 'thing': 622, 'naughty': 421, 'cuddlin': 157, 'young': 715, 'fit': 237, 'cmt': 134, 'jose': 334, 'clients': 129, 'south': 578, 'bay': 73, 'area': 57, 'message': 398, 'fantasy': 220, 'ass': 61, 'long': 369, 're': 507, 'shirt': 557, 'tie': 627, 'weird': 678, 'lol': 367, 'single': 563, '130': 1, 'lbs': 347, 'travel': 643, 'jewelry': 330, 'making': 380, 'see': 538, 'will': 693, 'respond': 521, 'respect': 519, 'responsible': 524, 'understand': 651, '44': 13, 'latina': 345, 'knight': 337, 'friendship': 251, 'physically': 470, 'intellectually': 318, 'meaningful': 393, 'conversations': 148, 'lightest': 356, 'funny': 256, 'games': 258, 'normal': 429, 'dude': 193, 'happens': 281, 'attraction': 66, 'come': 138, 'box': 99, 'car': 109, 'somewhere': 576, 'lousy': 374, 'hear': 287, 'science': 536, 'stuff': 600, 'poetry': 481, 'sociology': 571, 'novel': 435, 'smart': 568, 'progressive': 499, 'sense': 544, 'humor': 306, 'especially': 207, 'amidst': 45, 'existentially': 214, 'anyone': 52, 'friday': 247, 'dance': 162, 'lesson': 352, 'party': 462, 'jack': 329, 'london': 368, 'square': 585, 'flyer': 241, 'every': 211, 'haven': 285, 'gone': 268, 'yet': 712, 'expand': 215, 'circle': 122, 'flakey': 238, 'race': 502, 'important': 310, 'attitude': 65, 'belay': 81, 'climb': 130, 'ironworks': 324, '30': 6, '8am': 19, 'accountable': 22, 'bouldering': 97, 'why': 691, 'posting': 486, 'won': 699, 'response': 523, 'close': 131, 'although': 40, 'committed': 139, 'relationship': 513, 'feel': 225, 'empty': 200, 'void': 667, 'which': 684, 'ceases': 113, 'motivation': 413}\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]] (50, 1048576)\n"
     ]
    }
   ],
   "source": [
    "print(X_transformed_count.toarray(),X_transformed_count.toarray().shape,countvectorizer_transformer.vocabulary_)\n",
    "print(X_transformed_tfidf.todense(),X_transformed_tfidf.todense().shape,tfidfvectorizer_transformer.vocabulary_)\n",
    "print(X_transformed_hash.todense(),X_transformed_hash.todense().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average score of count vectorizer is 36.1%\n",
      "the average score of tfidf vectorizer is 35.9%\n",
      "the average score of hash vectorizer is 42.1%\n"
     ]
    }
   ],
   "source": [
    "#构建、评估分类器\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,HashingVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "countvectorizer_multinomialnb_pipeline = Pipeline([('countvectorizer',CountVectorizer(stop_words = 'english')),\n",
    "                                   ('multinomialnb',MultinomialNB())])\n",
    "tfidfvectorizer_multinomialnb_pipeline = Pipeline([('tfidfvectorizer',TfidfVectorizer(stop_words = 'english')),\n",
    "                                             ('multinomialnb',MultinomialNB())])\n",
    "hashingvectorizer_multinomialnb_pipeline = Pipeline([('hashingvectorizer',HashingVectorizer(non_negative = True,stop_words = 'english')),\n",
    "                                               ('multinomialnb',MultinomialNB())])\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores_countvectorizer = cross_val_score(countvectorizer_multinomialnb_pipeline,X_rss,y,scoring = 'accuracy')\n",
    "scores_tfidfvectorizer = cross_val_score(tfidfvectorizer_multinomialnb_pipeline,X_rss,y,scoring = 'accuracy')\n",
    "scores_hashingvectorizer = cross_val_score(hashingvectorizer_multinomialnb_pipeline,X_rss,y,scoring = 'accuracy')\n",
    "import numpy as np\n",
    "print('the average score of count vectorizer is {0:.1f}%'.format(np.mean(scores_countvectorizer)*100))\n",
    "print('the average score of tfidf vectorizer is {0:.1f}%'.format(np.mean(scores_tfidfvectorizer)*100))\n",
    "print('the average score of hash vectorizer is {0:.1f}%'.format(np.mean(scores_hashingvectorizer)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#词根（stemming）还原与词形（lemmatization）还原\n",
    "text = ['He ate the sandwiches','Every sandwich was eaten by him']\n",
    "from nltk import word_tokenize\n",
    "text_tokenized = [word_tokenize(document) for document in text]\n",
    "from nltk.stem import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "text_tokenized_stemmed = [[porter_stemmer.stem(word) for word in document] for document in text_tokenized]\n",
    "from nltk import pos_tag\n",
    "text_tokenized_tagged = [pos_tag(document) for document in text_tokenized]\n",
    "text_tokenized_stemmed_tagged = [pos_tag(document) for document in text_tokenized_stemmed]\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(word,tag):\n",
    "    if tag[0].lower() in ['n','v']:\n",
    "        return lemmatizer.lemmatize(word,tag[0].lower())\n",
    "    else:\n",
    "        return word\n",
    "text_tokenized_lemmatized = [[lemmatize(word,tag) for word,tag in document] for document in \n",
    "                             text_tokenized_tagged]\n",
    "text_tokenized_stemmed_lemmatized = [[lemmatize(word,tag) for word,tag in document] for document in \n",
    "                                    text_tokenized_stemmed_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['He', 'ate', 'the', 'sandwiches'], ['Every', 'sandwich', 'was', 'eaten', 'by', 'him']]\n",
      "[['He', 'ate', 'the', 'sandwich'], ['everi', 'sandwich', 'wa', 'eaten', 'by', 'him']]\n",
      "[[('He', 'PRP'), ('ate', 'VBD'), ('the', 'DT'), ('sandwiches', 'NNS')], [('Every', 'DT'), ('sandwich', 'NN'), ('was', 'VBD'), ('eaten', 'VBN'), ('by', 'IN'), ('him', 'PRP')]]\n",
      "[[('He', 'PRP'), ('ate', 'VBD'), ('the', 'DT'), ('sandwich', 'NN')], [('everi', 'NN'), ('sandwich', 'NN'), ('wa', 'NN'), ('eaten', 'VBN'), ('by', 'IN'), ('him', 'PRP')]]\n",
      "[['He', 'eat', 'the', 'sandwich'], ['Every', 'sandwich', 'be', 'eat', 'by', 'him']]\n",
      "[['He', 'eat', 'the', 'sandwich'], ['everi', 'sandwich', 'wa', 'eat', 'by', 'him']]\n"
     ]
    }
   ],
   "source": [
    "print(text_tokenized)\n",
    "print(text_tokenized_stemmed)\n",
    "print(text_tokenized_tagged)\n",
    "print(text_tokenized_stemmed_tagged)\n",
    "print(text_tokenized_lemmatized)\n",
    "print(text_tokenized_stemmed_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#词根（stemming）还原与词形（还原）\n",
    "from nltk import word_tokenize\n",
    "X_tokenized = [word_tokenize(document) for document in X_rss]\n",
    "from nltk.stem import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "X_tokenized_stemmed = [[porter_stemmer.stem(word) for word in document] for document in X_tokenized]\n",
    "from nltk import pos_tag\n",
    "X_tokenized_tagged = [pos_tag(document) for document in X_tokenized]\n",
    "X_tokenized_stemmed_tagged = [pos_tag(document) for document in X_tokenized_stemmed]\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(word,tag):\n",
    "    if tag[0].lower() in ['n','v']:\n",
    "        return lemmatizer.lemmatize(word,tag[0].lower())\n",
    "    else:\n",
    "        return word\n",
    "X_tokenized_lemmatized = [[lemmatize(word,tag) for word,tag in document] for document in X_tokenized_tagged]\n",
    "X_tokenized_stemmed_lemmatized = [[lemmatize(word,tag) for word,tag in document] for document in X_tokenized_stemmed_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将单词还原为文本\n",
    "X_tokenized_lemmatized_text = []\n",
    "for document in X_tokenized_lemmatized:\n",
    "    sentence = ''\n",
    "    for word in document:\n",
    "        sentence += ' '\n",
    "        sentence += word\n",
    "    X_tokenized_lemmatized_text.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average score of count vectorizer is 47.7%\n",
      "the average score of tfidf vectorizer is 49.5%\n",
      "the average score of hash vectorizer is 46.1%\n"
     ]
    }
   ],
   "source": [
    "#构建、评估分类器\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "countvectorizer_multinomialnb_pipeline = Pipeline([('countvectorizer',CountVectorizer(stop_words = 'english')),\n",
    "                                   ('multinomialnb',MultinomialNB())])\n",
    "tfidfvectorizer_multinomialnb_pipeline = Pipeline([('tfidfvectorizer',TfidfVectorizer(stop_words = 'english')),\n",
    "                                             ('multinomialnb',MultinomialNB())])\n",
    "hashingvectorizer_multinomialnb_pipeline = Pipeline([('hashingvectorizer',HashingVectorizer(non_negative = True,stop_words = 'english')),\n",
    "                                               ('multinomialnb',MultinomialNB())])\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores_countvectorizer = cross_val_score(countvectorizer_multinomialnb_pipeline,X_tokenized_lemmatized_text,y,scoring = 'accuracy')\n",
    "scores_tfidfvectorizer = cross_val_score(tfidfvectorizer_multinomialnb_pipeline,X_tokenized_lemmatized_text,y,scoring = 'accuracy')\n",
    "scores_hashingvectorizer = cross_val_score(hashingvectorizer_multinomialnb_pipeline,X_tokenized_lemmatized_text,y,scoring = 'accuracy')\n",
    "import numpy as np\n",
    "print('the average score of count vectorizer is {0:.1f}%'.format(np.mean(scores_countvectorizer)*100))\n",
    "print('the average score of tfidf vectorizer is {0:.1f}%'.format(np.mean(scores_tfidfvectorizer)*100))\n",
    "print('the average score of hash vectorizer is {0:.1f}%'.format(np.mean(scores_hashingvectorizer)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
